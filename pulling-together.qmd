# Putting it together: forecast - analysis cycle

```{r}
#| echo: false
#| message: false
source("R/forest_model.R")
source("R/helpers.R")
source("R/particle_filter.R")
library(tidyverse)
library(lubridate)
```

## Example of the forecast and analysis cycle

### Step 1: Set initial conditions and parameter distributions

We will start our forecast - analysis cycle by running the model with a particle filter over a period of time in the past. This is designed to spin-up the model and set the initial states and parameters for a forecast.

```{r}
ens_members <- 100
sim_dates <- seq(Sys.Date()-365, Sys.Date()-1, by = "1 day")
```

```{r}
inputs <- get_historical_met(site = "TALL", sim_dates, use_mean = FALSE)
inputs_ensemble <- assign_met_ensembles(inputs, ens_members)
```

```{r}
params <- list()
params$alpha <- rep(0.02, ens_members)
params$SLA <- rep(4.74, ens_members)
params$leaf_frac <- rep(0.315, ens_members)
params$Ra_frac <- rep(0.5, ens_members)
params$Rbasal <- rep(0.002, ens_members)
params$Q10 <- rep(2.1, ens_members)
params$litterfall_rate <- rep(1/(2.0*365), ens_members) #Two year leaf lifespan
params$litterfall_start <- rep(200, ens_members)
params$litterfall_length<- rep(70, ens_members)
params$mortality <- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)
params$sigma.leaf <- rep(0.1, ens_members) #0.01 
params$sigma.stem <- rep(1, ens_members) #0.01 ## wood biomass
params$sigma.soil <- rep(1, ens_members)# 0.01
params <- as.data.frame(params)
```

```{r}
obs <- read_csv("data/site_carbon_data.csv", show_col_types = FALSE)
state_init <- rep(NA, 3)

state_init[1] <- obs |> 
  filter(variable == "lai",
         datetime %in% sim_dates) |> 
  na.omit() |> 
  slice(1) |> 
  mutate(observation = observation / (mean(params$SLA) * 0.1)) |> 
  pull(observation)

state_init[2] <- obs |> 
  filter(variable == "wood") |> 
  na.omit() |> 
  slice(1) |> 
  pull(observation)

state_init[3] <- obs |> 
  filter(variable == "som") |> 
  na.omit() |>  
  slice(1) |> 
  pull(observation)
```

```{r}
#Set initial conditions
forecast <- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs
forecast[1, , 1] <- rnorm(ens_members, state_init[1], sd = 0.1)
forecast[1, , 2] <- rnorm(ens_members, state_init[2], sd = 10)
forecast[1, , 3] <- rnorm(ens_members, state_init[2], sd = 20)

wt <- array(1, dim = c(length(sim_dates), ens_members))
```

```{r}
fit_params_table <- read_csv("data/saved_parameter_chain.csv", show_col_types = FALSE) |> 
  pivot_wider(names_from = parameter, values_from = value)

num_pars <- 2
fit_params <- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))
samples <- sample(1:nrow(fit_params_table), size = ens_members, replace = TRUE)
fit_params[1, , 1] <- fit_params_table$alpha[samples]
fit_params[1, , 2] <- fit_params_table$Rbasal[samples]
```

```{r}
for(t in 2:length(sim_dates)){
  
  fit_params[t, , 1] <- rnorm(ens_members, fit_params[t-1, ,1], sd = 0.0005)
  fit_params[t, , 2] <- rnorm(ens_members, fit_params[t-1, ,2], sd = 0.00005)
  
  params$alpha  <- fit_params[t, , 1]
  params$Rbasal  <- fit_params[t, , 2]
  
  forecast[t, , ]  <- forest_model(t, 
                                   states = matrix(forecast[t-1 , , 1:3], nrow = ens_members) , 
                                   parms = params, 
                                   inputs = matrix(inputs_ensemble[t , , ], nrow = ens_members))
  
  analysis <- particle_filter(t, forecast, obs, sim_dates, wt, fit_params)
  
  forecast <- analysis$forecast
  fit_params <- analysis$fit_params
  wt <- analysis$wt
}
```

```{r}
save(analysis, file = "data/PF_analysis_0.Rdata")
```

```{r}
forecast_weighted <- array(NA, dim = c(length(sim_dates), ens_members, 12))
params_weighted <- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))
for(t in 1:length(sim_dates)){
  wt_norm <-  wt[t, ]/sum(wt[t, ])
  resample_index <- sample(1:ens_members, ens_members, replace = TRUE, prob = wt_norm ) 
  forecast_weighted[t, , ] <- forecast[t, resample_index, 1:12] 
  params_weighted[t, , ] <- fit_params[t,resample_index, ] 
}
output_df <- output_to_df(forecast_weighted, sim_dates, sim_name = "parameter_unc")
parameter_df <- parameters_to_df(params_weighted, sim_dates, sim_name = "parameter_unc", param_names = c("alpha","Rbasal"))
```

```{r}
#| warning: false
#| echo: false
output_df |> 
  summarise(median = median(prediction),
            upper = quantile(prediction, 0.95, na.rm = TRUE),
            lower = quantile(prediction, 0.05, na.rm = TRUE), .by = c("datetime", "variable")) |> 
  left_join(obs, by = c("datetime", "variable")) |> 
  filter(variable %in% c("nee","wood","som","lai")) |> 
  ggplot(aes(x = datetime)) +
  geom_ribbon(aes(ymin = upper, ymax = lower), alpha = 0.7) +
  geom_line(aes(y = median)) +
  geom_point(aes(y = observation), color = "red") +
  facet_wrap(~variable, scale = "free") +
  theme_bw()
```

### Step 2: Generate forecaset

Now we are going to generate a forecast that started from our 1 year spin up completed above. First load the analysis data into memory so that we can access the initial conditions and parameters in it.

```{r}
load("data/PF_analysis_0.Rdata")
```

This example introduces the concept of a "look back". This is stepping back in the past to restart the particle filter so that it can assimilate any data that has been make avialable since the last time that time period was run through the particle filter. The look back is important because many observations have delays before becoming avialable (called latency). NEON flux data can have a 5 day to 1.5 month latency. MODIS LAI is a 8 day average so has an 8-day latency.

Here we use a look back of 14-days. Since our forecast horizon is 30-days in the future, the total days of the simulation is 14 + 30. Our reference_datetime is today.

We will find the first day of our simulation (14 days ago) in the last saved analysis and use this for initial conditions.

```{r}
look_back <- 14
horizon <- 30
reference_datetime <- Sys.Date()
sim_dates <- seq(reference_datetime - look_back, length.out = look_back + horizon, by = "1 day")
index <- which(analysis$sim_dates == sim_dates[1])
```

The use of the loop back requires combining the "historial" weather and future weather into a single input data frame.

```{r}
inputs_past <- get_historical_met(site = "TALL", sim_dates[1:(look_back-2)], use_mean = FALSE)
inputs_future <- get_forecast_met(site = "TALL", sim_dates[(look_back-1):length(sim_dates)], use_mean = FALSE)
inputs <- bind_rows(inputs_past, inputs_future)
```

The combined weather drivers look like the following. The vertical line is where the look back period transitions to the future.

```{r}
#| echo: false
  ggplot(inputs, aes(x = datetime, y = prediction, group = parameter)) +
  geom_line() +
  geom_vline(aes(xintercept = reference_datetime)) +
  facet_wrap(~variable, scales = "free") +
  theme_bw()
```

```{r}
num_pars <- 2
fit_params <- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))
samples <- sample(1:nrow(fit_params_table), size = ens_members, replace = TRUE)
fit_params[1, , 1] <- analysis$fit_params[index, ,1]
fit_params[1, , 2] <- analysis$fit_params[index, ,1]
```

```{r}
#Set initial conditions
forecast <- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs
forecast[1, , 1] <- analysis$forecast[index, ,1]
forecast[1, , 2] <- analysis$forecast[index, ,2]
forecast[1, , 3] <- analysis$forecast[index, ,3]

wt <- array(1, dim = c(length(sim_dates), ens_members))
wt[1, ] <- analysis$wt[index, ]
```

```{r}
for(t in 2:length(sim_dates)){
  
  fit_params[t, , 1] <- rnorm(ens_members, fit_params[t-1, ,1], sd = 0.0005)
  fit_params[t, , 2] <- rnorm(ens_members, fit_params[t-1, ,2], sd = 0.00005)
  
  params$alpha  <- fit_params[t, , 1]
  params$Rbasal  <- fit_params[t, , 2]
  
  if(t > 1){
  
  forecast[t, , ]  <- forest_model(t, 
                                   states = matrix(forecast[t-1 , , 1:3], nrow = ens_members) , 
                                   parms = params, 
                                   inputs = matrix(inputs_ensemble[t , , ], nrow = ens_members))
  }
  
  analysis <- particle_filter(t, forecast, obs, sim_dates, wt, fit_params)
  
  forecast <- analysis$forecast
  fit_params <- analysis$fit_params
  wt <- analysis$wt
}
```

### Step 3: Save forecast and data assimiliation output

Convert forecast to a dataframe.

```{r}
output_df <- output_to_df(forecast, sim_dates, sim_name = "simple_forest_model")
```

Visualize forecast. The vertical line is where the look back period transitions to the future.

```{r}
#| echo: false
#| warning: false

output_df |> 
  summarise(median = median(prediction),
            upper = quantile(prediction, 0.95, na.rm = TRUE),
            lower = quantile(prediction, 0.05, na.rm = TRUE), 
            .by = c("datetime", "variable")) |> 
  left_join(obs, by = c("datetime", "variable")) |> 
  filter(variable %in% c("nee","wood","som","lai")) |> 
  ggplot(aes(x = datetime)) +
  geom_ribbon(aes(ymin = upper, ymax = lower), alpha = 0.7) +
  geom_line(aes(y = median)) +
  geom_point(aes(y = observation), color = "red") +
  geom_vline(aes(xintercept = reference_datetime)) +
  facet_wrap(~variable, scale = "free") +
  theme_bw()
```

Save the states and weights for use as initial conditions in the next forecast.

```{r}
save(analysis, file = "data/PF_analysis_1.Rdata")
```

Convert to the format required by the NEON Ecological Forecasting Challenge

```{r}
efi_output <- output_df |> 
  mutate(datetime = as_datetime(datetime),
         duration = "P1D",
         project_id = "neon4cast",
         site_id = "TALL",
         family = "ensemble",
         reference_datetime = as_datetime(reference_datetime)) |> 
  rename(parameter = ensemble) |> 
  filter(datetime >= reference_datetime) 
```

Write the forecast to a csv file

```{r}
file_name <- paste0("data/terrestrial_daily-", reference_datetime, "-simple_forest_model.csv")
write_csv(efi_output, file_name)
```

and submit to the Challenge

```{r}
#| eval: false
neon4cast::submit(file_name, ask = FALSE)
```

### Step 4: Repeat Steps 2 -3

Wait for a day to pass and then use yesterday's analysis today for initial conditions and parameter distributions

```{r}
load("data/PF_analysis_1.Rdata")
```

## Code example of forecast analysis cycle implimented in a more complex forest model

<https://github.com/mdietze/FluxCourseForecast>
